<resources>
    <string name="app_name">ffmpegdemos</string>
    <string name="content">
        正确的摘要不一定要按照逐字来匹配参考序列。
        你可以想像，对于同样的新闻文章，两个人可能在风格、单
        词或句子顺序上产生不尽相同的摘要，但仍然认为摘要是好的。
        教师强迫算法的问题是：一旦产生了前几个单词，训练就会被误导：
        严格遵守一个官方正确的摘要，但不能适应一个潜在正确但不同的开头。新闻太长不想看？深度解析MetaMind文本摘要新研究
机器之心发表于机器之心订阅
303
选自MetaMind

作者：Romain Paulus、Caiming Xiong、Richard Socher

机器之心编译

参与：Jane W、Cindy、吴攀

去年四月被 Salesforce 收购的 MetaMind 仍然在继续进行自然语言领域的前沿研究。近日，其研究博客发布了一篇文章，详细介绍了一种用于文本摘要提取的深度强化模型（deep reinforced model），机器之心对这篇博客进行编译介绍，并在文后附带了相关的研究论文摘要。论文链接：https://arxiv.org/abs/1705.04304机器之心发布过的 MetaMind 的其它文章还有《MetaMind 发布论文：借助动态记忆网络 DMN 让机器更好地像人类般推理》和《MetaMind 深度解读 NLP 研究：如何让机器学习跳读》。

近几十年来，获取新信息的方式发生了根本性变化，也带来了越来越多挑战。信息的获取已不再是瓶颈；瓶颈在于我们是否有能力紧跟信息的步伐。我们都必须通过越来越多的阅读来获取关于工作、新闻和社交媒体的最新进展。我们研究了人工智能在信息大潮中帮助人们提高工作能力的方法——答案之一是让算法自动归纳长文本。

怎样训练能够产生长句、连贯和有意义的摘要的模型仍然是一个有待解决的研究问题。事实上，即使是最先进的深度学习算法，生成任何长文本也是很困难的。为了使模型能够成功地生成摘要，我们引入了两个独立的改进：一个更加语境化的词生成模型和一种通过强化学习（RL）训练摘要模型的新方法。

两种训练方法的结合使得系统能够创建相关且高可读性的多语句长文本（例如新闻文章）摘要，并在之前的基础上实现了显著的提升。我们的算法可以对各种不同类型的文本和摘要长度进行训练。在本文中，我们介绍了我们的模型的主要贡献，并概述了文本摘要特有的自然语言挑战。


图 1：我们的模型的示例——由新闻文章生成多语句摘要。对于每个生成的词，模型重点关注输入的特定词和之前生成的输出。

提取式摘要（Extractive Summarization）与抽象式摘要（Abstractive Summarization）

自动摘要模型可以通过以下两种方法实现：通过提取或抽象。提取式模型执行「复制和粘贴」操作：它们选择输入文档的相关短语并连接它们以形成摘要。它们非常稳健，因为它们使用直接从原文中提取的已有自然语言短语，但是由于不能使用新词或连接词，它们缺乏灵活性。它们也不能像人一样改述。相反，抽象式模型基于实际的「抽象」内容生成摘要：它们可以使用原文中没有出现的词。这使得它们有更多的潜力来产生流畅和连贯的摘要，但因为需要模型生成连贯的短语和连接词，这也是一个更难的问题。

虽然抽象式模型在理论上更强大，但在实践中也常出现错误。在生成的摘要中，典型的错误包括不连贯、不相关或重复的短语，特别是在尝试创建长文本输出时。从已有模型来看，它们缺乏一般连贯性、意识流动性和可读性。在本任务中，我们解决了这些问题，并设计了一个更稳健和更连贯的抽象式摘要模型。

为了理解我们的新抽象式模型，我们首先定义基本构建块（building block），然后介绍我们新的训练方式。

用编码器-解码器模型读取和生成文本

循环神经网络（RNN）能够处理可变长度的序列（例如文本），并为每个短语计算有用的表征（或隐藏状态）。网络逐一处理序列的每个元素（在这种情况下，即每个词）；对于序列中的每个新输入，网络通过该输入和之前隐藏状态的函数输出新的隐藏状态。从这个角度讲，在每个词处计算的隐藏状态是所有之前读到的单词的函数输出。

作者：Romain Paulus、Caiming Xiong、Richard Socher

机器之心编译

参与：Jane W、Cindy、吴攀

去年四月被 Salesforce 收购的 MetaMind 仍然在继续进行自然语言领域的前沿研究。近日，其研究博客发布了一篇文章，详细介绍了一种用于文本摘要提取的深度强化模型（deep reinforced model），机器之心对这篇博客进行编译介绍，并在文后附带了相关的研究论文摘要。论文链接：https://arxiv.org/abs/1705.04304机器之心发布过的 MetaMind 的其它文章还有《MetaMind 发布论文：借助动态记忆网络 DMN 让机器更好地像人类般推理》和《MetaMind 深度解读 NLP 研究：如何让机器学习跳读》。

近几十年来，获取新信息的方式发生了根本性变化，也带来了越来越多挑战。信息的获取已不再是瓶颈；瓶颈在于我们是否有能力紧跟信息的步伐。我们都必须通过越来越多的阅读来获取关于工作、新闻和社交媒体的最新进展。我们研究了人工智能在信息大潮中帮助人们提高工作能力的方法——答案之一是让算法自动归纳长文本。

怎样训练能够产生长句、连贯和有意义的摘要的模型仍然是一个有待解决的研究问题。事实上，即使是最先进的深度学习算法，生成任何长文本也是很困难的。为了使模型能够成功地生成摘要，我们引入了两个独立的改进：一个更加语境化的词生成模型和一种通过强化学习（RL）训练摘要模型的新方法。

两种训练方法的结合使得系统能够创建相关且高可读性的多语句长文本（例如新闻文章）摘要，并在之前的基础上实现了显著的提升。我们的算法可以对各种不同类型的文本和摘要长度进行训练。在本文中，我们介绍了我们的模型的主要贡献，并概述了文本摘要特有的自然语言挑战。


图 1：我们的模型的示例——由新闻文章生成多语句摘要。对于每个生成的词，模型重点关注输入的特定词和之前生成的输出。

提取式摘要（Extractive Summarization）与抽象式摘要（Abstractive Summarization）

自动摘要模型可以通过以下两种方法实现：通过提取或抽象。提取式模型执行「复制和粘贴」操作：它们选择输入文档的相关短语并连接它们以形成摘要。它们非常稳健，因为它们使用直接从原文中提取的已有自然语言短语，但是由于不能使用新词或连接词，它们缺乏灵活性。它们也不能像人一样改述。相反，抽象式模型基于实际的「抽象」内容生成摘要：它们可以使用原文中没有出现的词。这使得它们有更多的潜力来产生流畅和连贯的摘要，但因为需要模型生成连贯的短语和连接词，这也是一个更难的问题。

虽然抽象式模型在理论上更强大，但在实践中也常出现错误。在生成的摘要中，典型的错误包括不连贯、不相关或重复的短语，特别是在尝试创建长文本输出时。从已有模型来看，它们缺乏一般连贯性、意识流动性和可读性。在本任务中，我们解决了这些问题，并设计了一个更稳健和更连贯的抽象式摘要模型。

为了理解我们的新抽象式模型，我们首先定义基本构建块（building block），然后介绍我们新的训练方式。

用编码器-解码器模型读取和生成文本

循环神经网络（RNN）能够处理可变长度的序列（例如文本），并为每个短语计算有用的表征（或隐藏状态）。网络逐一处理序列的每个元素（在这种情况下，即每个词）；对于序列中的每个新输入，网络通过该输入和之前隐藏状态的函数输出新的隐藏状态。从这个角度讲，在每个词处计算的隐藏状态是所有之前读到的单词的函数输出。

作者：Romain Paulus、Caiming Xiong、Richard Socher

机器之心编译

参与：Jane W、Cindy、吴攀

去年四月被 Salesforce 收购的 MetaMind 仍然在继续进行自然语言领域的前沿研究。近日，其研究博客发布了一篇文章，详细介绍了一种用于文本摘要提取的深度强化模型（deep reinforced model），机器之心对这篇博客进行编译介绍，并在文后附带了相关的研究论文摘要。论文链接：https://arxiv.org/abs/1705.04304机器之心发布过的 MetaMind 的其它文章还有《MetaMind 发布论文：借助动态记忆网络 DMN 让机器更好地像人类般推理》和《MetaMind 深度解读 NLP 研究：如何让机器学习跳读》。

近几十年来，获取新信息的方式发生了根本性变化，也带来了越来越多挑战。信息的获取已不再是瓶颈；瓶颈在于我们是否有能力紧跟信息的步伐。我们都必须通过越来越多的阅读来获取关于工作、新闻和社交媒体的最新进展。我们研究了人工智能在信息大潮中帮助人们提高工作能力的方法——答案之一是让算法自动归纳长文本。

怎样训练能够产生长句、连贯和有意义的摘要的模型仍然是一个有待解决的研究问题。事实上，即使是最先进的深度学习算法，生成任何长文本也是很困难的。为了使模型能够成功地生成摘要，我们引入了两个独立的改进：一个更加语境化的词生成模型和一种通过强化学习（RL）训练摘要模型的新方法。

两种训练方法的结合使得系统能够创建相关且高可读性的多语句长文本（例如新闻文章）摘要，并在之前的基础上实现了显著的提升。我们的算法可以对各种不同类型的文本和摘要长度进行训练。在本文中，我们介绍了我们的模型的主要贡献，并概述了文本摘要特有的自然语言挑战。


图 1：我们的模型的示例——由新闻文章生成多语句摘要。对于每个生成的词，模型重点关注输入的特定词和之前生成的输出。

提取式摘要（Extractive Summarization）与抽象式摘要（Abstractive Summarization）

自动摘要模型可以通过以下两种方法实现：通过提取或抽象。提取式模型执行「复制和粘贴」操作：它们选择输入文档的相关短语并连接它们以形成摘要。它们非常稳健，因为它们使用直接从原文中提取的已有自然语言短语，但是由于不能使用新词或连接词，它们缺乏灵活性。它们也不能像人一样改述。相反，抽象式模型基于实际的「抽象」内容生成摘要：它们可以使用原文中没有出现的词。这使得它们有更多的潜力来产生流畅和连贯的摘要，但因为需要模型生成连贯的短语和连接词，这也是一个更难的问题。

虽然抽象式模型在理论上更强大，但在实践中也常出现错误。在生成的摘要中，典型的错误包括不连贯、不相关或重复的短语，特别是在尝试创建长文本输出时。从已有模型来看，它们缺乏一般连贯性、意识流动性和可读性。在本任务中，我们解决了这些问题，并设计了一个更稳健和更连贯的抽象式摘要模型。

为了理解我们的新抽象式模型，我们首先定义基本构建块（building block），然后介绍我们新的训练方式。

用编码器-解码器模型读取和生成文本

循环神经网络（RNN）能够处理可变长度的序列（例如文本），并为每个短语计算有用的表征（或隐藏状态）。网络逐一处理序列的每个元素（在这种情况下，即每个词）；对于序列中的每个新输入，网络通过该输入和之前隐藏状态的函数输出新的隐藏状态。从这个角度讲，在每个词处计算的隐藏状态是所有之前读到的单词的函数输出。

作者：Romain Paulus、Caiming Xiong、Richard Socher

机器之心编译

参与：Jane W、Cindy、吴攀

去年四月被 Salesforce 收购的 MetaMind 仍然在继续进行自然语言领域的前沿研究。近日，其研究博客发布了一篇文章，详细介绍了一种用于文本摘要提取的深度强化模型（deep reinforced model），机器之心对这篇博客进行编译介绍，并在文后附带了相关的研究论文摘要。论文链接：https://arxiv.org/abs/1705.04304机器之心发布过的 MetaMind 的其它文章还有《MetaMind 发布论文：借助动态记忆网络 DMN 让机器更好地像人类般推理》和《MetaMind 深度解读 NLP 研究：如何让机器学习跳读》。

近几十年来，获取新信息的方式发生了根本性变化，也带来了越来越多挑战。信息的获取已不再是瓶颈；瓶颈在于我们是否有能力紧跟信息的步伐。我们都必须通过越来越多的阅读来获取关于工作、新闻和社交媒体的最新进展。我们研究了人工智能在信息大潮中帮助人们提高工作能力的方法——答案之一是让算法自动归纳长文本。

怎样训练能够产生长句、连贯和有意义的摘要的模型仍然是一个有待解决的研究问题。事实上，即使是最先进的深度学习算法，生成任何长文本也是很困难的。为了使模型能够成功地生成摘要，我们引入了两个独立的改进：一个更加语境化的词生成模型和一种通过强化学习（RL）训练摘要模型的新方法。

两种训练方法的结合使得系统能够创建相关且高可读性的多语句长文本（例如新闻文章）摘要，并在之前的基础上实现了显著的提升。我们的算法可以对各种不同类型的文本和摘要长度进行训练。在本文中，我们介绍了我们的模型的主要贡献，并概述了文本摘要特有的自然语言挑战。


图 1：我们的模型的示例——由新闻文章生成多语句摘要。对于每个生成的词，模型重点关注输入的特定词和之前生成的输出。

提取式摘要（Extractive Summarization）与抽象式摘要（Abstractive Summarization）

自动摘要模型可以通过以下两种方法实现：通过提取或抽象。提取式模型执行「复制和粘贴」操作：它们选择输入文档的相关短语并连接它们以形成摘要。它们非常稳健，因为它们使用直接从原文中提取的已有自然语言短语，但是由于不能使用新词或连接词，它们缺乏灵活性。它们也不能像人一样改述。相反，抽象式模型基于实际的「抽象」内容生成摘要：它们可以使用原文中没有出现的词。这使得它们有更多的潜力来产生流畅和连贯的摘要，但因为需要模型生成连贯的短语和连接词，这也是一个更难的问题。

虽然抽象式模型在理论上更强大，但在实践中也常出现错误。在生成的摘要中，典型的错误包括不连贯、不相关或重复的短语，特别是在尝试创建长文本输出时。从已有模型来看，它们缺乏一般连贯性、意识流动性和可读性。在本任务中，我们解决了这些问题，并设计了一个更稳健和更连贯的抽象式摘要模型。

为了理解我们的新抽象式模型，我们首先定义基本构建块（building block），然后介绍我们新的训练方式。

用编码器-解码器模型读取和生成文本

循环神经网络（RNN）能够处理可变长度的序列（例如文本），并为每个短语计算有用的表征（或隐藏状态）。网络逐一处理序列的每个元素（在这种情况下，即每个词）；对于序列中的每个新输入，网络通过该输入和之前隐藏状态的函数输出新的隐藏状态。从这个角度讲，在每个词处计算的隐藏状态是所有之前读到的单词的函数输出。

作者：Romain Paulus、Caiming Xiong、Richard Socher

机器之心编译

参与：Jane W、Cindy、吴攀

去年四月被 Salesforce 收购的 MetaMind 仍然在继续进行自然语言领域的前沿研究。近日，其研究博客发布了一篇文章，详细介绍了一种用于文本摘要提取的深度强化模型（deep reinforced model），机器之心对这篇博客进行编译介绍，并在文后附带了相关的研究论文摘要。论文链接：https://arxiv.org/abs/1705.04304机器之心发布过的 MetaMind 的其它文章还有《MetaMind 发布论文：借助动态记忆网络 DMN 让机器更好地像人类般推理》和《MetaMind 深度解读 NLP 研究：如何让机器学习跳读》。

近几十年来，获取新信息的方式发生了根本性变化，也带来了越来越多挑战。信息的获取已不再是瓶颈；瓶颈在于我们是否有能力紧跟信息的步伐。我们都必须通过越来越多的阅读来获取关于工作、新闻和社交媒体的最新进展。我们研究了人工智能在信息大潮中帮助人们提高工作能力的方法——答案之一是让算法自动归纳长文本。

怎样训练能够产生长句、连贯和有意义的摘要的模型仍然是一个有待解决的研究问题。事实上，即使是最先进的深度学习算法，生成任何长文本也是很困难的。为了使模型能够成功地生成摘要，我们引入了两个独立的改进：一个更加语境化的词生成模型和一种通过强化学习（RL）训练摘要模型的新方法。

两种训练方法的结合使得系统能够创建相关且高可读性的多语句长文本（例如新闻文章）摘要，并在之前的基础上实现了显著的提升。我们的算法可以对各种不同类型的文本和摘要长度进行训练。在本文中，我们介绍了我们的模型的主要贡献，并概述了文本摘要特有的自然语言挑战。


图 1：我们的模型的示例——由新闻文章生成多语句摘要。对于每个生成的词，模型重点关注输入的特定词和之前生成的输出。

提取式摘要（Extractive Summarization）与抽象式摘要（Abstractive Summarization）

自动摘要模型可以通过以下两种方法实现：通过提取或抽象。提取式模型执行「复制和粘贴」操作：它们选择输入文档的相关短语并连接它们以形成摘要。它们非常稳健，因为它们使用直接从原文中提取的已有自然语言短语，但是由于不能使用新词或连接词，它们缺乏灵活性。它们也不能像人一样改述。相反，抽象式模型基于实际的「抽象」内容生成摘要：它们可以使用原文中没有出现的词。这使得它们有更多的潜力来产生流畅和连贯的摘要，但因为需要模型生成连贯的短语和连接词，这也是一个更难的问题。

虽然抽象式模型在理论上更强大，但在实践中也常出现错误。在生成的摘要中，典型的错误包括不连贯、不相关或重复的短语，特别是在尝试创建长文本输出时。从已有模型来看，它们缺乏一般连贯性、意识流动性和可读性。在本任务中，我们解决了这些问题，并设计了一个更稳健和更连贯的抽象式摘要模型。

为了理解我们的新抽象式模型，我们首先定义基本构建块（building block），然后介绍我们新的训练方式。

用编码器-解码器模型读取和生成文本

循环神经网络（RNN）能够处理可变长度的序列（例如文本），并为每个短语计算有用的表征（或隐藏状态）。网络逐一处理序列的每个元素（在这种情况下，即每个词）；对于序列中的每个新输入，网络通过该输入和之前隐藏状态的函数输出新的隐藏状态。从这个角度讲，在每个词处计算的隐藏状态是所有之前读到的单词的函数输出。
    </string>
</resources>
